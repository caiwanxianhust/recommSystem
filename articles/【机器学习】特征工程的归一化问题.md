## 1 问题背景

**对于一个机器学习问题，数据和特征往往决定了结果的上限，而模型的选择及模型优化方式则是在逐步接近这个上限。**特征工程，服名思义，是对原始数据进行一系列工程处理，将原始数据提炼为特征，作为模型的输入来使用 。从本质上来讲，特征工程是一个表示和展现数据的过程。在实际工作中，特征工程旨在去除原始数据中的杂质和冗余，设计更高效的特征来刻画业务问题与预测模型之间的关系 。  

为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同指标之间具有可比性。例如，分析一个人的身高和体重对健康的影响，如果使用米$(m)$和千克$(kg)$作为单位，那么身高特征会在$1.5～1.8m$的数值范围内，体重特征会在$40 ～ 100kg$ 的范围内，分析出来的结果显然会倾向于数值差别比较大的体重特征对健康程度的影响更大。可见量纲的选择对分析结果影响很大，这显然是不合理的。怎么解决这个问题呢？可以对原始数据（身高和体重）进行**特征归一化（Normalization）**处理，使各指标处于同一数值量级，然后再输入模型进行分析。  

## 2 常用归一化方法

对数值类型的特征做归一化可以将所有的特征都统一到一个大致相同的数值区间内。最常用的方法主要是两种：极值归一化、标准化。

### 2.1 极值归一化（MinMax Normalization）

将数据转换到某个固定区间内，通常这个区间是`[0, 1]`，广义的讲，可以是各种区间，比如图像中可能会映射到`[0,255]`，其他情况可能映射到`[-1,1]`等等。下面给出常用公式：
$$
X_{norm} = \frac{X_i - X_{min}}{X_{max} - X_{min}}, \quad X_{norm} \in [0,1]
$$
其中，$X_{max}$与$X_{min}$分别为原始数据的最大值和最小值。

这种归一化方法比较适用在**数值比较集中**的情况。但是，如果数值不稳定，存在极端最大值和极端最小值，很容易使得归一化结果不稳定，使得后续使用效果也不稳定，**实际使用中可以用经验常量值来替代$X_{max}$和$X_{min}$**。

### 2.2 标准化（Standardization，也称Z-Score Normalization）

将数据变换为均值为0，标准差为1的分布。切记，并非一定是正态分布！具体来说，假设原始特征均值为$\mu$，标准差为$\sigma$，那么标准化公式定义为：
$$
X_{norm} = \frac{X - \mu}{\sigma}
$$

这里需要强调一点，**标准化并不会改变数据的分布类型**。它只是改变数据的均值、标准差（当然，严格的说，均值和标准差变了，分布也是变了，但分布种类依然没变，原来是啥类型，现在还是啥类型)，标准化后的分布并不一定是标准正态分布，其完全取决于原始数据是什么分布。举个例子，笔者随机生成了10w个服从$\gamma(2,0.5) $的样本点(你可以替换成任意非正态分布，比如卡方等等)，称这个原始数据为$\gamma _0$，分布如下图所示：

```python
a, b = 2, .5
x = np.linspace(0, 10, 100002)[1:-1]
fig, ax = plt.subplots(figsize=(14,9))
plt.plot(x, stats.gamma.pdf(x, a, b), c='tab:blue',label=r'$\alpha=%.1f,\ \beta=%.1f$' % (a, b))

plt.xlim(0, 10)
plt.ylim(0, .4)
plt.legend()
```

![原始分布图](https://mmbiz.qpic.cn/mmbiz_png/GJUG0H1sS5oMp5Vs77ofXdyyaHfaYKiag8bLqjGg5zBZuSHY7JFsBy0FKMlnibSYASbgWFZ0faNk50ia7nKZwAK3g/0?wx_fmt=png)

原始样本$\gamma_0$的均值和方差分别为0.0999和0.1225，对这个数据做标准化，称标准化后的数据为$\gamma_1$，分布如下：

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
y_scaled = scaler.fit_transform(y.reshape(-1,1))

fig, ax = plt.subplots(figsize=(14,9))
plt.plot(x, y_scaled, c='tab:blue')
plt.xlim(0, 10)
plt.ylim(-1, 2.5)
```

![标准化后的分布](https://mmbiz.qpic.cn/mmbiz_png/GJUG0H1sS5oMp5Vs77ofXdyyaHfaYKiagxxejoPpfRbXq1xlLokCzsNTjHrFd2r0icNJoPTBxo3gYyic83IvbLU9w/0?wx_fmt=png)

可以看到，标准化后的分布显然不是正态分布（正态分布是对称的），与标准化之前相比，分布类型没有变化。由此可见，标准化并不会改变分布类型。

### 2.3 极值归一化和标准化二者的异同点

**相同点：**

- 本质上都是对数据的线性变换，二者都是不会改变原始数据排列顺序；
- 令$\alpha = X_{min}, \beta = X_{max} - X_{min}$，（显然数据给定后两者都是一个常数），那么极值归一化的公式可以表示为：$X_{norm} = \frac{X_i - \alpha}{\beta}$，形式上等同于标准化，只是标准化采用的$\alpha$是均值$\mu$，采用的$\beta$是标准差$\sigma$，本质上都是按$\beta$进行缩放，平移$\alpha / \beta$个单位。

**差异：**

- 极值归一化会严格的限定变换后数据的范围，比如按最大最小值处理的它的范围严格在`[0,1]`之间；
  而标准化就没有严格的区间，变换后的数据没有范围，只是其均值是0，标准差为1。

- 极值归一化对数据的缩放比例仅仅和极值有关，就是说比如100个数，除去极大值和极小值其他数据都更换掉，缩放比例是不变的；反观，对于标准化而言，如果除去极大值和极小值其他数据都更换掉，那么均值和标准差大概率会改变，这时候，缩放比例就改变了。

## 3 为什么要归一化？选择哪种归一化方式？

### 3.1 为什么要归一化？

- 统计建模中，如回归模型，自变量$X$量纲不一致导致了回归系数（权重）无法直接解读或者错误解读；需要将$X$都处理到统一量纲下，这样才具备可比性，便于分析哪个特征对结果影响更大；

- 机器学习任务和统计学任务中有很多地方要用到“距离”的计算，比如PCA，KNN，kmeans等等，如果计算欧式距离，不同维度量纲不同可能会导致距离的计算依赖于量纲较大的那些特征而得到不合理的结果；

- 参数估计时使用梯度下降，在使用梯度下降的方法求解最优化问题时， 归一化后可以加快梯度下降的求解速度，即提升模型的收敛速度。如下图，$x_1$的取值为`[0,2000]`，而$x_2$的取值为`[1,5]`，假如只有这两个特征，对其进行优化时，会得到一个窄长的椭圆形，导致在梯度下降时，梯度的方向为垂直等高线的方向而走之字形路线，这样会使迭代很慢，相比之下，右图的迭代就会很快。

  ![](https://mmbiz.qpic.cn/mmbiz_png/GJUG0H1sS5oMp5Vs77ofXdyyaHfaYKiagonOml0M7VsDnuicRF2kpQicyvTgQnjrFLltbKBB7gOHCEweW9L8FuHEQ/0?wx_fmt=png)

### 3.2 什么时候用极值归一化，什么时候用标准化？

- 如果对处理后的数据范围有严格要求，那肯定是极值归一化；
- 如果数据不为稳定，存在极端的最大最小值，不要用极值归一化。
- 在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，标准化表现更好；
- 在不涉及距离度量、协方差计算的时候，可以使用极值归一化方法。但是，标准化是机器学习中更通用的手段，经验表明其效果通常优于极值归一化。

## 4 归一化的使用场景

- 在实际应用中，通过梯度下降法求解的模型通常是需要归一化的包括线性回归、逻辑回归、支持向量机、 神经网络等模型 。 
- 概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率。比如决策树类模型，特征缩放并不会改变其分裂点的位置和特征选择结果。

